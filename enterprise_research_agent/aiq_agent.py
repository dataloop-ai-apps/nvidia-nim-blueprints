"""
NVIDIA AIQ Enterprise Research Agent - Dataloop Service Runner

Pipeline flow:
  Input → [Init] → [AIQ Agent]
                      ├─ "web_search" → [Tavily Web Search] → [AIQ Agent] (cycle)
                      ├─ "ask_rag"    → [RAG Retrieval]     → [AIQ Agent] (cycle)
                      └─ "generate_report" → [NIM Llama 3.3 70B Instruct] (end)

The Agent node is the brain: it plans, reflects, and decides which tool to use next.
Web Search and RAG are external tool nodes that cycle back to the Agent.
The final report is generated by the NIM Llama predict node using nearestItems context.
"""

import dtlpy as dl
import os
import json
import logging
import re
import time
import tempfile
from typing import Optional

from langchain_nvidia_ai_endpoints import ChatNVIDIA
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.utils.json import parse_json_markdown
from tavily import TavilyClient

from enterprise_research_agent.prompts import (
    QUERY_WRITER_INSTRUCTIONS,
    SUMMARIZER_INSTRUCTIONS,
    REPORT_EXTENDER,
    REFLECTION_INSTRUCTIONS,
    FINALIZE_REPORT,
    RAG_QUERY_INSTRUCTIONS,
)

logger = logging.getLogger('[AIQ-Enterprise-Research]')

# Default configuration
DEFAULT_NUM_REFLECTIONS = 2
DEFAULT_NUM_QUERIES = 3
DEFAULT_REASONING_MODEL = "nvidia/llama-3.3-nemotron-super-49b-v1"
DEFAULT_NVIDIA_BASE_URL = "https://integrate.api.nvidia.com/v1"


class AIQEnterpriseAgent(dl.BaseServiceRunner):
    """Service runner for the NVIDIA AIQ Enterprise Research Agent pipeline."""

    def __init__(self):
        nvidia_api_key = os.environ.get("NGC_API_KEY")
        if nvidia_api_key is None:
            raise ValueError("Missing NGC_API_KEY environment variable.")

        tavily_api_key = os.environ.get("TAVILY_API_KEY")
        if tavily_api_key is None:
            raise ValueError("Missing TAVILY_API_KEY environment variable.")

        self.tavily_client = TavilyClient(api_key=tavily_api_key)

        base_url = os.environ.get("NVIDIA_BASE_URL", DEFAULT_NVIDIA_BASE_URL)

        # Reasoning LLM for planning, summarization, reflection
        self.reasoning_llm = ChatNVIDIA(
            model=os.environ.get("REASONING_MODEL", DEFAULT_REASONING_MODEL),
            api_key=nvidia_api_key,
            base_url=base_url,
            temperature=0.2,
            max_tokens=4096,
        )

    # ─── Helpers ──────────────────────────────────────────────────────────────

    def _get_state(self, item: dl.Item) -> dict:
        """Get the aiq_state from item metadata, or return empty dict."""
        return item.metadata.get('user', {}).get('aiq_state', {})

    def _set_state(self, item: dl.Item, state: dict) -> dl.Item:
        """Save aiq_state to item metadata."""
        item.metadata.setdefault('user', {})
        item.metadata['user']['aiq_state'] = state
        item = item.update(system_metadata=True)
        return item

    def _get_main_item(self, item: dl.Item) -> dl.Item:
        """Get the original PromptItem from a temp item's metadata."""
        main_item_id = item.metadata.get('user', {}).get('main_item')
        if main_item_id:
            return dl.items.get(item_id=main_item_id)
        return item

    def _is_temp_item(self, item: dl.Item) -> bool:
        """Check if this is a temp item created by the agent."""
        return 'main_item' in item.metadata.get('user', {})

    def _create_temp_item(self, main_item: dl.Item, content: str, name: str) -> dl.Item:
        """Create a temp item in a hidden folder for passing data between nodes."""
        safe_name = re.sub(r'[^\w\s-]', '', name)[:50].strip().replace(' ', '_')
        filename = f"{safe_name}.txt"

        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:
            f.write(content)
            local_path = f.name

        try:
            temp_item = main_item.dataset.items.upload(
                local_path=local_path,
                remote_path=f"/.dataloop/aiq_temp_{main_item.id[:8]}/",
                remote_name=filename,
                overwrite=True,
                item_metadata={
                    "user": {
                        "main_item": main_item.id
                    }
                }
            )
            return temp_item
        finally:
            os.remove(local_path)

    def _invoke_llm(self, prompt_text: str, system_prompt: str = "You are an expert research assistant. Think step by step.") -> str:
        """Call the reasoning LLM."""
        prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("human", "{input}"),
        ])
        chain = prompt | self.reasoning_llm
        result = chain.invoke({"input": prompt_text})
        return result.content

    def _parse_json_response(self, text: str) -> dict | list | None:
        """Parse JSON from LLM response, handling <think> tags and markdown blocks."""
        cleaned = text
        while "<think>" in cleaned and "</think>" in cleaned:
            start = cleaned.find("<think>")
            end = cleaned.find("</think>") + len("</think>")
            cleaned = cleaned[:start] + cleaned[end:]
        if "<think>" in cleaned:
            cleaned = cleaned[:cleaned.find("<think>")]

        try:
            return parse_json_markdown(cleaned)
        except Exception:
            json_match = re.search(r'```(?:json)?\s*(.*?)\s*```', cleaned, re.DOTALL)
            if json_match:
                try:
                    return json.loads(json_match.group(1))
                except json.JSONDecodeError:
                    pass
            try:
                return json.loads(cleaned.strip())
            except json.JSONDecodeError:
                logger.warning(f"Could not parse JSON from response: {cleaned[:200]}")
                return None

    def _extract_params_from_prompt(self, prompt_text: str) -> dict:
        """Extract research parameters from the user's input prompt.

        Expected format:
        Topic: <topic>
        Report Organization: <organization>
        Number of queries: <int> (optional)
        Number of reflections: <int> (optional)
        """
        params = {
            'topic': '',
            'report_organization': '',
            'num_queries': DEFAULT_NUM_QUERIES,
            'num_reflections': DEFAULT_NUM_REFLECTIONS,
        }
        lines = prompt_text.strip().split('\n')

        for line in lines:
            lower = line.strip().lower()
            if lower.startswith('topic:'):
                params['topic'] = line.split(':', 1)[1].strip()
            elif lower.startswith('number of queries:'):
                try:
                    params['num_queries'] = int(line.split(':', 1)[1].strip())
                except ValueError:
                    pass
            elif lower.startswith('number of reflections:'):
                try:
                    params['num_reflections'] = int(line.split(':', 1)[1].strip())
                except ValueError:
                    pass

        # Extract report organization
        org_lines = []
        capture = False
        for line in lines:
            lower = line.strip().lower()
            if lower.startswith('report organization:'):
                capture = True
                rest = line.split(':', 1)[1].strip()
                if rest:
                    org_lines.append(rest)
                continue
            elif capture and any(lower.startswith(k) for k in
                                ['number of queries:', 'number of reflections:', 'topic:']):
                capture = False
                continue
            if capture:
                org_lines.append(line)

        params['report_organization'] = '\n'.join(org_lines).strip()

        # If no structured format, use whole text as topic
        if not params['topic'] and not params['report_organization']:
            params['topic'] = prompt_text.strip()
            params['report_organization'] = "Introduction, Key Findings, Analysis, Conclusion"

        return params

    # ─── Tavily Search Helpers ────────────────────────────────────────────────

    def _tavily_search(self, query: str) -> dict:
        """Perform a web search using Tavily API."""
        try:
            return self.tavily_client.search(
                query, max_results=5, include_raw_content=True, topic="general"
            )
        except Exception as e:
            logger.error(f"Tavily search error: {e}")
            return {"results": []}

    def _format_search_results(self, search_results: list) -> str:
        """Format search results into a source string."""
        formatted = "Sources:\n\n"
        for i, result in enumerate(search_results, 1):
            formatted += f"Source {i}: {result.get('title', 'N/A')}\n"
            formatted += f"URL: {result.get('url', 'N/A')}\n"
            formatted += f"Content: {result.get('content', 'N/A')}\n"
            raw = result.get('raw_content', '') or ''
            if len(raw) > 4000:
                raw = raw[:4000] + "... [truncated]"
            if raw:
                formatted += f"Full content: {raw}\n"
            formatted += "\n"
        return formatted.strip()

    def _collect_citations(self, search_results: list) -> list:
        """Extract citation strings from search results."""
        return [f"- {r.get('title', 'N/A')}: {r.get('url', 'N/A')}" for r in search_results]

    # ─── Pipeline Node Functions ──────────────────────────────────────────────

    def init_research(self, item: dl.Item, rag_pipeline_id: str = None):
        """
        Pipeline node: Init (ROOT)
        Receives pipeline variables and stores them in item metadata so the agent
        and tool nodes can access them throughout the pipeline execution.

        Args:
            item: The input PromptItem.
            rag_pipeline_id: Optional ID of a configured 'RAG Pipeline NVIDIA Blueprint' instance.
                             If provided, the agent will execute this pipeline for RAG retrieval.
                             If empty/None, only web search will be used.
        """
        logger.info("=== AIQ Init node ===")
        state = self._get_state(item)
        if rag_pipeline_id:
            # Validate the RAG pipeline exists and is active
            try:
                rag_pipeline = dl.pipelines.get(pipeline_id=rag_pipeline_id)
                if not rag_pipeline.installed:
                    logger.warning(
                        f"RAG pipeline '{rag_pipeline.name}' (ID: {rag_pipeline_id}) is not installed/active. "
                        f"RAG retrieval will be skipped. Install the pipeline to enable RAG."
                    )
                else:
                    state['rag_pipeline_id'] = rag_pipeline_id
                    logger.info(f"RAG pipeline validated and active: {rag_pipeline.name} (ID: {rag_pipeline_id})")
            except Exception as e:
                logger.warning(
                    f"Could not find RAG pipeline with ID '{rag_pipeline_id}': {e}. "
                    f"RAG retrieval will be skipped. Proceeding with web search only."
                )
        else:
            logger.info("No RAG pipeline configured - web search only mode")
        self._set_state(item, state)
        return item

    def run_agent(self, item: dl.Item, context: dl.Context, progress: dl.Progress):
        """
        Pipeline node: NVIDIA AIQ Agent (with action outputs)

        The brain of the pipeline. Decides which tool to call next.

        Actions:
          - "web_search": send query to Tavily Web Search node
          - "ask_rag": send query to RAG Retrieval node
          - "generate_report": send PromptItem to NIM Llama for final report
        """
        logger.info("=== AIQ Agent node ===")

        # Determine if this is a temp item (returning from a tool) or the original PromptItem
        if self._is_temp_item(item):
            main_item = self._get_main_item(item)
            source = item.metadata.get('user', {}).get('source', 'unknown')
            logger.info(f"Received item from tool: {source}")
        else:
            main_item = item
            source = 'init'
            logger.info("First call - initializing research")

        state = self._get_state(main_item)

        # ── FIRST CALL: Initialize research ──
        if source == 'init' and not state:
            prompt_item = dl.PromptItem.from_item(main_item)
            prompts_json = prompt_item.to_json()['prompts']
            first_key = list(prompts_json.keys())[0]
            prompt_text = prompts_json[first_key][0]['value']

            params = self._extract_params_from_prompt(prompt_text)
            logger.info(f"Research topic: {params['topic']}")

            # Generate search queries using reasoning LLM
            query_prompt = QUERY_WRITER_INSTRUCTIONS.format(
                number_of_queries=params['num_queries'],
                topic=params['topic'],
                report_organization=params['report_organization']
            )
            query_response = self._invoke_llm(query_prompt)
            queries = self._parse_json_response(query_response)
            if not queries or not isinstance(queries, list):
                queries = [{"query": params['topic'], "report_section": "All", "rationale": "Main topic"}]

            logger.info(f"Generated {len(queries)} search queries")

            # Initialize state
            state = {
                'topic': params['topic'],
                'report_organization': params['report_organization'],
                'num_reflections': params['num_reflections'],
                'num_queries': params['num_queries'],
                'iteration': 0,
                'running_summary': '',
                'citations': [],
                'all_rag_context': '',
                'all_web_results': '',
                'pending_queries': queries,
                'completed_queries': [],
            }
            main_item = self._set_state(main_item, state)

            # Create temp item for web search with queries
            query_texts = [q.get('query', str(q)) if isinstance(q, dict) else str(q) for q in queries]
            temp_item = self._create_temp_item(
                main_item,
                content=json.dumps(query_texts),
                name="web_search_queries"
            )
            temp_item.metadata.setdefault('user', {})
            temp_item.metadata['user']['source'] = 'agent_to_web_search'
            temp_item.metadata['user']['search_queries'] = query_texts
            temp_item.update(system_metadata=True)

            progress.update(action="web_search")
            return temp_item

        # ── RETURNING FROM WEB SEARCH ──
        elif source == 'web_search':
            web_results = item.metadata.get('user', {}).get('search_results', [])
            web_citations = item.metadata.get('user', {}).get('citations', [])
            web_source_str = item.metadata.get('user', {}).get('source_str', '')

            logger.info(f"Web search returned {len(web_results)} results")

            # Accumulate results
            state['citations'] = state.get('citations', []) + web_citations
            state['all_web_results'] = state.get('all_web_results', '') + '\n\n' + web_source_str

            # Summarize or extend report
            if not state.get('running_summary'):
                summary_prompt = SUMMARIZER_INSTRUCTIONS.format(
                    report_organization=state['report_organization'],
                    source=web_source_str
                )
                state['running_summary'] = self._invoke_llm(summary_prompt)
                logger.info("Initial summary generated")
            else:
                extend_prompt = REPORT_EXTENDER.format(
                    report=state['running_summary'],
                    source=web_source_str
                )
                state['running_summary'] = self._invoke_llm(extend_prompt)
                logger.info("Report extended with new sources")

            main_item = self._set_state(main_item, state)

            # Check if RAG is available (rag_pipeline_id in state)
            rag_pipeline_id = state.get('rag_pipeline_id')
            if rag_pipeline_id and state['iteration'] == 0:
                # First iteration: also ask RAG for context
                rag_query_prompt = RAG_QUERY_INSTRUCTIONS.format(
                    topic=state['topic'],
                    current_summary=state['running_summary'][:2000]
                )
                rag_query = self._invoke_llm(rag_query_prompt)
                state['current_rag_query'] = rag_query.strip()
                main_item = self._set_state(main_item, state)

                temp_item = self._create_temp_item(main_item, content=rag_query, name="rag_query")
                temp_item.metadata.setdefault('user', {})
                temp_item.metadata['user']['source'] = 'agent_to_rag'
                temp_item.metadata['user']['rag_query'] = rag_query.strip()
                temp_item.update(system_metadata=True)

                progress.update(action="ask_rag")
                return temp_item

            # No RAG or not first iteration - go to reflection
            return self._do_reflection(main_item, state, progress)

        # ── RETURNING FROM RAG ──
        elif source == 'rag':
            rag_context = item.metadata.get('user', {}).get('rag_context', '')
            logger.info(f"RAG returned context length: {len(rag_context)}")

            if rag_context:
                state['all_rag_context'] = state.get('all_rag_context', '') + '\n\n' + rag_context
                # Extend report with RAG context
                extend_prompt = REPORT_EXTENDER.format(
                    report=state['running_summary'],
                    source=f"Document Corpus Context:\n{rag_context}"
                )
                state['running_summary'] = self._invoke_llm(extend_prompt)
                logger.info("Report extended with RAG context")

            main_item = self._set_state(main_item, state)
            return self._do_reflection(main_item, state, progress)

        else:
            # Unknown source - treat as returning from init
            logger.warning(f"Unknown source: {source}, treating as pass-through")
            progress.update(action="generate_report")
            return main_item

    def _do_reflection(self, main_item: dl.Item, state: dict, progress: dl.Progress) -> dl.Item:
        """Internal: perform reflection and decide next action."""
        iteration = state.get('iteration', 0)
        max_reflections = state.get('num_reflections', DEFAULT_NUM_REFLECTIONS)

        # Reflect on the current report
        reflection_prompt = REFLECTION_INSTRUCTIONS.format(
            topic=state['topic'],
            report_organization=state['report_organization'],
            report=state['running_summary']
        )
        reflection_response = self._invoke_llm(reflection_prompt)
        reflection = self._parse_json_response(reflection_response)

        state['iteration'] = iteration + 1

        if state['iteration'] < max_reflections and reflection:
            # More research needed
            follow_up_query = reflection.get('query', state['topic']) if isinstance(reflection, dict) else state['topic']
            logger.info(f"Reflection {state['iteration']}/{max_reflections}: more research needed - {follow_up_query}")

            state['pending_queries'] = [{"query": follow_up_query}]
            main_item = self._set_state(main_item, state)

            # Create temp item for targeted web search
            temp_item = self._create_temp_item(main_item, content=follow_up_query, name="followup_search")
            temp_item.metadata.setdefault('user', {})
            temp_item.metadata['user']['source'] = 'agent_to_web_search'
            temp_item.metadata['user']['search_queries'] = [follow_up_query]
            temp_item.update(system_metadata=True)

            progress.update(action="web_search")
            return temp_item
        else:
            # Research complete - prepare for report generation
            logger.info(f"Research complete after {state['iteration']} iterations. Preparing for report generation.")
            return self._prepare_for_report(main_item, state, progress)

    def _prepare_for_report(self, main_item: dl.Item, state: dict, progress: dl.Progress) -> dl.Item:
        """Internal: compile research and prepare original PromptItem for NIM Llama."""

        # Compile all research into a comprehensive document
        citations_str = "\n".join(state.get('citations', []))
        research_doc = f"""# Enterprise Research: {state['topic']}

## Report Structure
{state['report_organization']}

## Research Summary
{state['running_summary']}

## Web Search Findings
{state.get('all_web_results', 'No web search results available.')}

## Document Corpus Context (RAG)
{state.get('all_rag_context', 'No RAG context available.')}

## Sources
{citations_str if citations_str else 'No sources collected.'}
"""

        # Upload research document to hidden folder
        safe_topic = re.sub(r'[^\w\s-]', '', state['topic'])[:40].strip().replace(' ', '_')
        filename = f"research_{safe_topic}.md"

        with tempfile.NamedTemporaryFile(mode='w', suffix='.md', delete=False, encoding='utf-8') as f:
            f.write(research_doc)
            local_path = f.name

        try:
            research_item = main_item.dataset.items.upload(
                local_path=local_path,
                remote_path="/.dataloop/aiq_research/",
                remote_name=filename,
                overwrite=True,
            )
            logger.info(f"Uploaded research document: {research_item.id}")
        finally:
            os.remove(local_path)

        # Set nearestItems on the original PromptItem so NIM Llama picks up the context
        prompt_item = dl.PromptItem.from_item(main_item)
        last_prompt = prompt_item.prompts[-1]

        if not hasattr(last_prompt, 'metadata') or last_prompt.metadata is None:
            last_prompt.metadata = {}
        last_prompt.metadata['nearestItems'] = [research_item.id]

        # Save the updated PromptItem
        prompt_item.to_item(main_item)
        logger.info("Set nearestItems on PromptItem for NIM Llama context")

        progress.update(action="generate_report")
        return main_item

    # ─── Web Search Node ─────────────────────────────────────────────────────

    def web_search(self, item: dl.Item):
        """
        Pipeline node: Tavily Web Search

        Reads search queries from item metadata, executes Tavily searches,
        stores results in item metadata, returns to Agent.
        """
        logger.info("=== Tavily Web Search node ===")

        queries = item.metadata.get('user', {}).get('search_queries', [])
        if not queries:
            logger.warning("No search queries found in item metadata")
            item.metadata.setdefault('user', {})
            item.metadata['user']['source'] = 'web_search'
            item.metadata['user']['search_results'] = []
            item.metadata['user']['citations'] = []
            item.metadata['user']['source_str'] = ''
            item.update(system_metadata=True)
            return item

        logger.info(f"Executing {len(queries)} search queries")

        all_results = []
        all_citations = []
        for query in queries:
            query_text = query.get('query', query) if isinstance(query, dict) else str(query)
            search_result = self._tavily_search(query_text)
            results = search_result.get('results', [])
            all_results.extend(results)
            all_citations.extend(self._collect_citations(results))

        source_str = self._format_search_results(all_results)
        logger.info(f"Collected {len(all_results)} total results")

        # Store results in item metadata for the Agent to read
        item.metadata.setdefault('user', {})
        item.metadata['user']['source'] = 'web_search'
        item.metadata['user']['search_results'] = [
            {'title': r.get('title', ''), 'url': r.get('url', ''), 'content': r.get('content', '')}
            for r in all_results
        ]
        item.metadata['user']['citations'] = all_citations
        item.metadata['user']['source_str'] = source_str
        item.update(system_metadata=True)

        return item

    # ─── RAG Retrieval Node ──────────────────────────────────────────────────

    def rag_retrieve(self, item: dl.Item):
        """
        Pipeline node: RAG Retrieval

        Executes the existing 'RAG Pipeline NVIDIA Blueprint' (nim-rag-bp) to retrieve
        information from the document corpus. The RAG pipeline handles embedding,
        retrieval, and response generation internally.

        Flow:
          1. Create a PromptItem with the RAG query
          2. Execute the configured RAG pipeline with the PromptItem
          3. Wait for the pipeline execution to complete
          4. Read the LLM response from the PromptItem's annotations
          5. Store the RAG context in the temp item metadata
          6. Return the temp item to the Agent node
        """
        logger.info("=== RAG Retrieval node ===")

        main_item_id = item.metadata.get('user', {}).get('main_item')
        if main_item_id:
            main_item = dl.items.get(item_id=main_item_id)
        else:
            main_item = item

        state = self._get_state(main_item)
        rag_query = item.metadata.get('user', {}).get('rag_query', '') or state.get('current_rag_query', '')
        rag_pipeline_id = state.get('rag_pipeline_id')

        if not rag_pipeline_id or not rag_query:
            logger.info("RAG pipeline not configured or no query - returning empty context")
            item.metadata.setdefault('user', {})
            item.metadata['user']['source'] = 'rag'
            item.metadata['user']['rag_context'] = ''
            item.update(system_metadata=True)
            return item

        try:
            # 1. Get the configured RAG pipeline and verify it is active
            rag_pipeline = dl.pipelines.get(pipeline_id=rag_pipeline_id)
            if not rag_pipeline.installed:
                logger.warning(f"RAG pipeline '{rag_pipeline.name}' is not active - skipping RAG retrieval")
                item.metadata.setdefault('user', {})
                item.metadata['user']['source'] = 'rag'
                item.metadata['user']['rag_context'] = ''
                item.update(system_metadata=True)
                return item
            logger.info(f"Using RAG pipeline: {rag_pipeline.name} (ID: {rag_pipeline_id})")

            # 2. Create a PromptItem with the RAG query
            safe_name = re.sub(r'[^\w\s-]', '', rag_query)[:40].strip().replace(' ', '_')
            prompt_item = dl.PromptItem(name=f"rag_{safe_name}_{main_item.id[:8]}")
            prompt_item.add(
                message={
                    "role": "user",
                    "content": [{"mimetype": dl.PromptType.TEXT, "value": rag_query}]
                }
            )

            # Upload the PromptItem to the same dataset as the main item
            rag_prompt_item = main_item.dataset.items.upload(
                prompt_item,
                remote_path="/.dataloop/aiq_rag_queries/",
                overwrite=True,
            )
            logger.info(f"Created RAG PromptItem: {rag_prompt_item.id}")

            # 3. Execute the RAG pipeline with the PromptItem as input
            execution = rag_pipeline.execute(
                execution_input={"item": [rag_prompt_item.id]}
            )
            logger.info(f"Started RAG pipeline execution: {execution.id}")

            # 4. Wait for the pipeline execution to complete
            max_wait_seconds = 300  # 5 minutes max
            poll_interval = 5
            elapsed = 0

            while elapsed < max_wait_seconds:
                time.sleep(poll_interval)
                elapsed += poll_interval

                # Check execution status
                success, response = dl.client_api.gen_request(
                    req_type="get",
                    path=f"/pipelines/{rag_pipeline_id}/executions/{execution.id}"
                )
                if success:
                    status = response.json().get('status', '')
                    logger.info(f"RAG pipeline execution status: {status} (elapsed: {elapsed}s)")
                    if status in ['success', 'completed']:
                        break
                    elif status in ['failed', 'error']:
                        logger.error(f"RAG pipeline execution failed: {response.json()}")
                        break
                else:
                    logger.warning(f"Could not check RAG execution status (elapsed: {elapsed}s)")

            # 5. Read the response from the PromptItem's annotations
            rag_context = ''
            try:
                # Refresh the PromptItem to get the latest annotations
                rag_prompt_item = dl.items.get(item_id=rag_prompt_item.id)
                updated_prompt = dl.PromptItem.from_item(rag_prompt_item)

                # The RAG pipeline's LLM node writes the response as the assistant message
                # in the PromptItem. Extract the last assistant response.
                prompts_json = updated_prompt.to_json().get('prompts', {})
                for key in prompts_json:
                    for msg in prompts_json[key]:
                        if msg.get('mimetype') == 'application/text' and msg.get('role', '') != 'user':
                            rag_context = msg.get('value', '')

                # Fallback: check annotations directly
                if not rag_context:
                    annotations = rag_prompt_item.annotations.list()
                    for annotation in annotations:
                        ann_data = annotation.to_json()
                        # Look for text/response annotations from the LLM
                        coordinates = ann_data.get('coordinates', {})
                        if isinstance(coordinates, dict):
                            for key in ['response', 'text', 'value']:
                                if key in coordinates:
                                    rag_context = str(coordinates[key])
                                    break
                        if rag_context:
                            break

                logger.info(f"RAG response length: {len(rag_context)}")

            except Exception as e:
                logger.error(f"Error reading RAG response: {e}")

        except Exception as e:
            logger.error(f"RAG pipeline execution error: {e}")
            rag_context = ''

        # 6. Store context in item metadata for the Agent to read
        item.metadata.setdefault('user', {})
        item.metadata['user']['source'] = 'rag'
        item.metadata['user']['rag_context'] = rag_context[:50000]  # Limit size for metadata
        item.update(system_metadata=True)

        return item
